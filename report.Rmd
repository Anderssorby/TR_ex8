---
title: "State space and SARIMA model analysis"
subtitle: "TMA4285 Time series models - Exercise 8"
author: Anders Christiansen Sørby, Edvard Hove
#bibliography: mybiblio.bib
header-includes:
  - \usepackage{dsfont}
  - \usepackage{bm}
  - \DeclareMathOperator*{\E}{\mathrm{E}}
  - \DeclareMathOperator*{\Var}{\mathrm{Var}}
  - \DeclareMathOperator*{\Cov}{\mathrm{Cov}}
output: pdf_document
keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("itsmr")
#install.packages("forecast")
library(forecast)
library(latex2exp)
library(itsmr)
```
\abstract{
}

# Introduction

In this project we will analyze a time series using two different models.

## The State space model

A possibly multivariate state space model consists of two equations. First the observation equation 
\begin{equation}
Y_t = G_t X_t + W_t, \hspace{5em} t \in \mathbb{N}
\end{equation}
where $W_t \sim WN(0, \{R_t\})$. Secondly there is the state equation 
\begin{equation}
X_{t+1} = F_t X_t + V_t, \hspace{5em} t\in \mathbb{N}
\end{equation}
where $V_t \sim WN(0, \{Q_t\})$.

## The SARIMA process

The seasonal ARIMA (SARIMA) process is a generalization of the ARIMA process can be defined as follows. Given two nonnegative integers $d$, and $D$, $\{X_t\}$ is a seasonal ARIMA$(p,d,q)\times(P,D,Q)_s$ with period $s$ if the differenced series $Y_t = (1-B)^d(1-B^s)^DX_t$ is a causal ARMA process defined by
\begin{equation}
\phi(B)\Phi(B^s)Y_t = \theta(B)\Theta(B^s)Z_t,\hspace{5em} \{Z_t\}\sim WN(0,\sigma^2)
\end{equation}
where $\phi(z)= 1-\phi_1 z-\ldots- \phi_p z^p$, $\Phi(z) = 1- \Phi_1 z - \ldots - \Phi_p z^p$, $\theta(z) = 1+ \theta_1 z + \ldots + \theta_q z^q$ and $\Theta(z) = 1- \Theta_1 z - \ldots - \Theta_p z^p$.

For causality for $Y_t$ to hold we need


# Theory

## Model parameter estimation

<!--
REPEATED ESTIMATION ON SIMULATED DATA COMES HERE:
We need to confirm that a given process is indeed an $ARMA(2,1)$
-->
##Model choice

Given a collection of models, the Akaike information criterion estimates the quality of each model relative to the other models. 
As such it is a means for model selection.
The AIC is defined as 
\begin{equation}
AIC = 2k - 2\ln(\hat L),
\end{equation}
\noindent where $k$ is the number of estimated parameters in the model and $\hat{L}$ is the model's likelihood function's maximum value.
The model with the lower AIC should be selected, rewarding goodness of fit while punishing overfitting.
```{r read, echo=FALSE}
x = read.table("airpass.txt")
x <- x[,1]
l <- length(x)
```

# Data analysis
The realization of our recieved process can be seen in the next plot.

```{r TSMplot, echo=FALSE}
plot(seq(1,l),x,type="l", xlab="t", ylab="X_t")
```

There is no obvious need for a transformation of the data to fit an ARMA model. Based on the plot it seems probable that both the mean and variance is constant with respect to time. 
<!--(DETTE HADDE VÆRT FINT Å DEMONSTRERE)-->

```{r lagplot, echo=FALSE,include=FALSE}
lag.plot(x, 9, diag.col = "red")
```

```{r arma}
ordr <-c(2,0,1)
seasonal <- c(1,2,3)

arima=arima(x,order=ordr,include.mean=FALSE)

phi1 <- arima$coef[1]
phi2 <- arima$coef[2]
ar <- c(phi1,phi2)

theta1 <- arima$coef[3]
ma <- c(theta1)

sigma2 <- arima$sigma2
sigma <- sqrt(sigma2)
```
```{r acvf,fig.cap="The sample autocorrelation $\\hat{\\rho}(h)$ \\label{fig:acf}",echo=FALSE}
lag.max <- 100
acf<-acf(x,lag.max=lag.max,plot=FALSE)
plot(acf,main="")
theoreticalacf <- ARMAacf(ar=ar,ma=ma,lag.max=(l-1))
lines(seq(0,(l-1)),theoreticalacf,col=2,lty=3)
```

It is conceivable, based on the sample autocorrelation function in figure \ref{fig:acf}, that there is some underlying seasonal lag. To check this we used the functions `decompose` and `stl` in R, both of which claims that the series is not periodic.  More importantly the sample ACF forces us to reject the hypothesis that the data is generated by an \(MA(q)\) model (atleast for \(q\leq 50\)). The sample partial autocorrelation function in figure \ref{fig:pacf} does not force us to reject that the data is generated by an \(AR(p)\) series. Both of these figures include a dotted red line indicating the theoretical functions for the $ARMA(2,1)$ model we find below.

```{r pacf,echo=FALSE,fig.cap="The sample partial autocorrelation $\\hat{\\alpha}(h)$ \\label{fig:pacf}"}
lag.max <- 100
pacf <- acf(x,type="partial",lag.max=lag.max,plot=FALSE)
theoreticalpacf <- ARMAacf(ar=ar,ma=ma,lag.max=(l-1),pacf=TRUE)
plot(pacf,main="")
lines(seq(1,(l-1)),theoreticalpacf,col=2,lty=3)
```


Using the function `arima` we get MLE parameter estimates for the underlying process. 
```{r arimamodel,echo=FALSE}
print(arima)
```



```{r prediction,fig.cap="A quantile-quantile plot of the estimated noise and a normal distribution\\label{fig:noise}", eval=FALSE}

```

## Model prediction
The projection \(P_n X_{n+h}\) is the linear combination of \(1,X_1,\hdots, X_n\) that forecasts \(X_{n+h}\) with minimum mean squared error. For a process with zero mean that is

\begin{equation}
P_n X_{n+h} = \sum_{i=1}^n a_i X_{n+h-i}
\end{equation}

\noindent such that

\begin{equation}
\frac{\partial}{\partial a_i} \E[(X_{n+h}-a_1 X_n - \dots - a_n X_1)^2] = 0, \hspace{1cm} i=1,\hdots,n.
\end{equation}

\noindent Evaluating the derivatives gives the equations

\begin{equation}
\E[(X_{n+h}-\sum_{i=1}^n a_i X_{n+1-i})X_{n+1-j}] = 0,  \hspace{1cm} j=1,\hdots,n
\end{equation}

\noindent which can be written in vector notation as 

\begin{equation}
\Gamma_n \bm{a}_n = \gamma_n(h)
\end{equation}

\noindent where 
\begin{equation}
\gamma_n = (\gamma(h),\gamma(h+1),\hdots,\gamma(h+n-1))',
\end{equation}

\noindent and
\begin{equation}
\Gamma_n = [\Cov(X_i,X_j)]_{i,j=1}^n.
\end{equation}

\noindent The expected prediction error is zero, and the expected mean square prediction error is given by

\begin{equation}
\E[(X_{n+h}-P_nX_{n+h})^2] = \gamma(0) - \bm{a}'\gamma_n(h).
\end{equation}

```{r onestep, echo=FALSE}
itsmrarma<-arma(x,p=2,q=1)

gamma<-aacvf(itsmrarma,499)
Gamma <- matrix(ncol=l,nrow=l)
for (i in 1:l){
  for (j in 1:l){
    Gamma[i,j] <- gamma[abs(i-j)+1]
  }
}

a <- vector("list",l-1)
for(i in 1:(l-1)){
  a[[i]] = solve(Gamma[1:i,1:i])%*%gamma[2:(i+1)]
}

xhat <-rep(0,l)
xhatse <- rep(gamma[1],l)
for (i in 2:l){
  xhat[i] <- t(a[[i-1]])%*%x[(i-1):1]
  xhatse[i] <- sqrt(gamma[1]-t(a[[i-1]])%*%gamma[2:i])
}
```

```{r onestepplot,fig.cap="One-step ahead predictions\\label{fig:onestep}",echo=FALSE}
plot(x,col=2,type="l",xlim=c(1,100),main="",ylab=TeX("X_t,\\hat{X}_t,Z_t"),xlab="Time")
lines(xhat,col=4)
U <- xhat+xhatse
L <- xhat-xhatse
xx = c(time(U), rev(time(U))); yy = c(L, rev(U))
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2))
```


In figure \ref{fig:onestep} we plot the process \(X_t\) in red along with the one-step ahead predictions \(\hat{X_t}\) in blue and the corresponding standard error in gray. The prediction tends to be slightly closer to the x-axis than the process itself, especially for the more extreme values. Intuitively this makes sense since the process' extreme values occur around extreme values for the underlying white noise, which the prediction expects to be zero. 

``` {r forecasting,fig.cap="Forecasted values of the series\\label{fig:forecast}",echo=FALSE}
fore <- predict(arima,20)
ts.plot(c(x,fore$pred),col=1:2,xlim=c(420,520),ylab=TeX("X_t"))
U <- fore$pred+fore$se
L <- fore$pred-fore$se
xx = c(time(U), rev(time(U))); yy = c(L, rev(U))
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2))
lines(fore$pred, type="p", col=2)
```

Using the same linear predictor as above, we can forecast the next 20 values of \(X_t\) given \(X_1,\hdots,X_500\). This is done in figure \ref{fig:forecast}. As is to be expected, the forecast seems to gradually approach the series mean of zero with an expected error that approaches \(\sigma_X=\sqrt{\gamma(0)}\approx `r sprintf('%.3f',sqrt(gamma[1]))`\).

# Discussion


```{r modelchoice, echo=FALSE,eval=FALSE}
af<-autofit(x,p=0:7,q=0:7)
#aa<-auto.arima(x,d=0)
```

# Conclusion



# Appendix

What follows is a complete copy of the computer code used by R markdown to produce the report.

```{r appendix,eval=FALSE,echo=TRUE}

```

\begin{thebibliography}{9}
\bibitem{brockwelldavies}
  Brockwell, Peter J., Davis, Richard A.,
  \textit{Introduction to Time Series and Forecasting},
  2nd edition,
  2002.
\end{thebibliography}
