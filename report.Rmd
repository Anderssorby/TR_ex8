---
title: "State space and SARIMA model analysis"
subtitle: "TMA4285 Time series models - Exercise 8"
author: Anders Christiansen SÃ¸rby, Edvard Hove
#bibliography: mybiblio.bib
header-includes:
  - \usepackage{dsfont}
  - \usepackage{bm}
  - \DeclareMathOperator*{\E}{\mathrm{E}}
  - \DeclareMathOperator*{\Var}{\mathrm{Var}}
  - \DeclareMathOperator*{\Cov}{\mathrm{Cov}}
output: pdf_document
keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("itsmr")
#install.packages("forecast")
#install.packages("astsa")
library(forecast)
library(latex2exp)
library(itsmr)
library(astsa)
```
\abstract{
}

# Introduction

In this project we will analyze a time series using two different models, namely the state space and the SARIMA models. First we will define them. 

## The State space model

A possibly multivariate state space model consists of two equations. First the observation equation 
\begin{equation}
Y_t = G_t X_t + W_t, \hspace{5em} t \in \mathbb{N}
\end{equation}
where $W_t \sim WN(0, \{R_t\})$. Secondly there is the state equation 
\begin{equation}
X_{t+1} = F_t X_t + V_t, \hspace{5em} t\in \mathbb{N}
\end{equation}
where $V_t \sim WN(0, \{Q_t\})$. 

## The SARIMA process

The seasonal ARIMA (SARIMA) process is a generalization of the ARIMA process can be defined as follows. Given two nonnegative integers $d$, and $D$, $\{X_t\}$ is a seasonal ARIMA$(p,d,q)\times(P,D,Q)_s$ with period $s$ if the differenced series $Y_t = (1-B)^d(1-B^s)^DX_t$ is a causal ARMA process defined by
\begin{equation}
\label{eq:SARIMA}
\phi(B)\Phi(B^s)Y_t = \theta(B)\Theta(B^s)Z_t,\hspace{5em} \{Z_t\}\sim WN(0,\sigma^2)
\end{equation}
where $\phi(z)= 1-\phi_1 z-\ldots- \phi_p z^p$, $\Phi(z) = 1- \Phi_1 z - \ldots - \Phi_p z^p$, $\theta(z) = 1+ \theta_1 z + \ldots + \theta_q z^q$ and $\Theta(z) = 1- \Theta_1 z - \ldots - \Theta_p z^p$. This can also be reformulated to using only one polynomial on each side. 

For causality for $Y_t$ to hold we need that $\phi(z)\neq 0$ and $\Phi(z)\neq 0$ for $|z| \leq 1$. The SARIMA allows for randomness in the seasonal pattern and is therefore more practical in applications than classical decomposition models. 


# Theory

## Model parameter estimation and uncertainty

For the state space model we can assume that the initial state, ${X}_1$, observation $Y_0$, and the noice vectors $W_t$ and $V_t$ are jointly Gaussian. This simplifies paramter estimation greatly. We are going to need the Kalman prediction for state space models to estimate parameters and do prediction. For state space models the best linear predictors $\hat{X}_t:=\hat{E}[X_t|Y_{t-1},\ldots, Y_0]$ and correspnding error covariance matricies $\Omega_t:=E[({X}_t-\hat{{X}}_t)(X_t-\hat{{X}}_t)^T]$ are uniquely determined by the initial conditions $\hat{{X}}$

This gives us a conditional density
\begin{equation}
f_t({Y}_t | {Y}_{t-1}, \ldots,{Y}_0) = (2\pi)^{-nw/2} \left(\prod_{j=1}^n\det\Delta_j\right)^{-1/2} (2\pi)^{-w/2}(det\Delta_t)^{-1/2}\exp\left(-\frac{1}{2}I_t^T\Delta^{-1}I_t\right),
\end{equation}
and a likelihood function
\begin{equation}
L(\theta ; Y_{t-1}, \ldots, {Y}_0) =  \exp\left(-\frac{1}{2} \sum_{j=0}^n I_t^T \Delta_j^{-1} I_t \right).
\end{equation}

We can get the ML estimate for $\theta$ by maximizing this function. 

##Model choice

Given a collection of models, the Akaike information criterion estimates the quality of each model relative to the other models. 
As such it is a means for model selection.
The AIC is defined as 
\begin{equation}
AIC = 2k - 2\ln(\hat L),
\end{equation}
\noindent where $k$ is the number of estimated parameters in the model and $\hat{L}$ is the model's likelihood function's maximum value.
The model with the lower AIC should be selected, rewarding goodness of fit while punishing overfitting.
```{r read, echo=FALSE}
x = read.table("airpass.txt")
x <- x[,1]
l <- length(x)
```

# Data analysis

```{r TSMplot, echo=FALSE, fig.cap="\\label{fig:timeseries} The data set AirPassengers, showing monthly totals of international airline passengers."}
plot(seq(1,l),x,type="l", xlab="t", ylab=TeX("X_t"))
```

The realization of the process from `AirPassengers` can be seen in figure \ref{fig:timeseries}.
This process is clearly in need of some transformation.
First of all it has a clear upwards trend, secondly there is some seasonal behaviour, and thirdly the variance seems to be increasing.
A commonly used approach to solve the third problem is to consider the logarithm of the data.

```{r logTSMplot, echo=FALSE, fig.cap="\\label{fig:logts} The data set AirPassengers, after taking the logarithm of the data."}
lx <- log(x)
plot(seq(1,l),lx,type="l", xlab="t", ylab=TeX("log X_t"))
```

\noindent This is done in figure \ref{fig:logts}.
The transformation seems to have stabilized the variance fairly well.
Next we difference the logged data to remove the apparent linear trend.

```{r difflogTSMplot, echo=FALSE, fig.cap="\\label{fig:dlts} The data set AirPassengers, showing monthly totals of international airline passengers."}
dlx<- diff(lx)
plot(seq(1,l-1),dlx,type="l", xlab="t", ylab=TeX("$\\nabla$ log X_t"))
```

\noindent The differenced and logged data can be seen in figure \ref{fig:dlts}.

```{r dlxACF, echo=FALSE, fig.cap="\\label{fig:ACFdlx} The sample autocorrelation function of the differenced logged data"}
Acf(dlx,main="")
```

\noindent After differencing the data still appears to have some seasonal effect.
It is natural to suspect a season of 12, since these are monthly data.
The ACF of $\nabla \log (X_t)$, as can be seen in figure \ref{fig:ACFdlx} has a clear peak at lag 12, thereby confirming these suspicions.
By applying a twelfth-order difference we attain what appears to be stationary data, as can be seen in figure \ref{fig:ddlxts}.
In terms of \eqref{eq:SARIMA}, we have are now considering a model where $d = 1, D = 1, s = 12$.


```{r ddlx, echo=FALSE,fig.cap="\\label{fig:ddlxts} The data from AirPassengers after transformation into an apparent stationary series."}
ddlx <- diff(dlx,12)
plot(seq(1,l-13),ddlx,type="l", xlab="t", ylab=TeX("$\\nabla_{12} \\nabla$ log X_t"))
```

```{r seasonal, echo=FALSE,fig.cap="\\label{fig:sacf} Sample ACF and PACF of $\\nabla_{12} \\nabla$ log X_t"}
test <- acf2(ddlx,50,main="")
```
To determine the seasonal components, as in \eqref{eq:SARIMA}, we consider the ACF and PACF of the transformed data.
These are shown in figure \ref{fig:sacf}.
At the seasonal lags, i.e. the lags which are multiples of $s$, the ACF cuts off after one season.
This suggests that a seasonal component on the form of an SMA(1), that is $P = 0, Q = 1$.
For the smaller lags, both the ACF and PACF gradually decrease in a way that suggests the within seasons part could be on the form of an ARMA(1,1), that is $p = 1, q = 1$. 
By using these estimates for $p,P,q$ and $Q$ we can now estimate the coefficients in the polynomials $\phi, \theta$ and $\Theta$.
Using the R function `arima` gives

```{r sarima1}
ordr <- c(1,1,1)
seasonal <- list(order= c(0,1,1), period = 12)
sarima1 <- arima(lx,,order=ordr, seasonal=seasonal)
sarima1
```

Since the `ar1` coefficient is less than one standard deviation away from 0, it is possible that this model is overfitting the non-seasonal component.
In order to get better estimates for the order of the polynomials, we use the R function `auto.arima` which uses the AICc to select the best from a range of models.

```{r autoarima}
slx <- ts(lx,frequency=12,start=c(1949,1))
sarima <- auto.arima(slx,d = NA, D = NA, max.p=4, max.q=4, max.P=2, max.Q=2,
                     max.order=20, max.d=2, max.D=1, start.p=1, start.q=1,
                     start.P=1, start.Q=1, stationary=FALSE, seasonal=TRUE,
                     trace=TRUE)
sarima
```

Hence the best estimates for $p, d, q, P, D$ and $Q$, assuming $s=12$, gives us an ARIMA(0,1,1)\texttimes$(0,1,1)_{12}$ model with coefficients as in the printout above.
Using the R function `sarima.for` we can forecast the logged data for the next year. 

```{r forecast, fig.cap="\\label{fig:forecast} The forecasted values for the next year in red, with standard errors in grey"}
forecast <- sarima.for(slx,12,0,1,1,0,1,1,12)
plot.ts(c(ts(exp(lx),frequency=12,start=c(1949,1)),ts(exp(forecast$pred),frequency=12,start=c(1962,1))),col=1:2)
```
```{r arma}
ordr <-c(2,0,1)
seasonal <- c(1,2,3)

arima=arima(x,order=ordr,include.mean=FALSE)

phi1 <- arima$coef[1]
phi2 <- arima$coef[2]
ar <- c(phi1,phi2)

theta1 <- arima$coef[3]
ma <- c(theta1)

sigma2 <- arima$sigma2
sigma <- sqrt(sigma2)
```
```{r acvf,fig.cap="The sample autocorrelation $\\hat{\\rho}(h)$ \\label{fig:acf}",echo=FALSE}
lag.max <- 100
acf<-acf(x,lag.max=lag.max,plot=FALSE)
plot(acf,main="")
theoreticalacf <- ARMAacf(ar=ar,ma=ma,lag.max=(l-1))
lines(seq(0,(l-1)),theoreticalacf,col=2,lty=3)
```

It is conceivable, based on the sample autocorrelation function in figure \ref{fig:acf}, that there is some underlying seasonal lag. To check this we used the functions `decompose` and `stl` in R, both of which claims that the series is not periodic.  More importantly the sample ACF forces us to reject the hypothesis that the data is generated by an \(MA(q)\) model (atleast for \(q\leq 50\)). The sample partial autocorrelation function in figure \ref{fig:pacf} does not force us to reject that the data is generated by an \(AR(p)\) series. Both of these figures include a dotted red line indicating the theoretical functions for the $ARMA(2,1)$ model we find below.

```{r pacf,echo=FALSE,fig.cap="The sample partial autocorrelation $\\hat{\\alpha}(h)$ \\label{fig:pacf}"}
lag.max <- 100
pacf <- acf(x,type="partial",lag.max=lag.max,plot=FALSE)
theoreticalpacf <- ARMAacf(ar=ar,ma=ma,lag.max=(l-1),pacf=TRUE)
plot(pacf,main="")
lines(seq(1,(l-1)),theoreticalpacf,col=2,lty=3)
```

Using the function `arima` we get MLE parameter estimates for the underlying process. 
```{r arimamodel,echo=FALSE}
print(arima)
```



```{r prediction,fig.cap="A quantile-quantile plot of the estimated noise and a normal distribution\\label{fig:noise}", eval=FALSE}

```

## Model prediction
The projection \(P_n X_{n+h}\) is the linear combination of \(1,X_1,\hdots, X_n\) that forecasts \(X_{n+h}\) with minimum mean squared error. For a process with zero mean that is

\begin{equation}
P_n X_{n+h} = \sum_{i=1}^n a_i X_{n+h-i}
\end{equation}

\noindent such that

\begin{equation}
\frac{\partial}{\partial a_i} \E[(X_{n+h}-a_1 X_n - \dots - a_n X_1)^2] = 0, \hspace{1cm} i=1,\hdots,n.
\end{equation}

\noindent Evaluating the derivatives gives the equations

\begin{equation}
\E[(X_{n+h}-\sum_{i=1}^n a_i X_{n+1-i})X_{n+1-j}] = 0,  \hspace{1cm} j=1,\hdots,n
\end{equation}

\noindent which can be written in vector notation as 

\begin{equation}
\Gamma_n \bm{a}_n = \gamma_n(h)
\end{equation}

\noindent where 
\begin{equation}
\gamma_n = (\gamma(h),\gamma(h+1),\hdots,\gamma(h+n-1))',
\end{equation}

\noindent and
\begin{equation}
\Gamma_n = [\Cov(X_i,X_j)]_{i,j=1}^n.
\end{equation}

\noindent The expected prediction error is zero, and the expected mean square prediction error is given by

\begin{equation}
\E[(X_{n+h}-P_nX_{n+h})^2] = \gamma(0) - \bm{a}'\gamma_n(h).
\end{equation}

```{r onestep, echo=FALSE}
itsmrarma<-arma(x,p=2,q=1)

gamma<-aacvf(itsmrarma,499)
Gamma <- matrix(ncol=l,nrow=l)
for (i in 1:l){
  for (j in 1:l){
    Gamma[i,j] <- gamma[abs(i-j)+1]
  }
}

a <- vector("list",l-1)
for(i in 1:(l-1)){
  a[[i]] = solve(Gamma[1:i,1:i])%*%gamma[2:(i+1)]
}

xhat <-rep(0,l)
xhatse <- rep(gamma[1],l)
for (i in 2:l){
  xhat[i] <- t(a[[i-1]])%*%x[(i-1):1]
  xhatse[i] <- sqrt(gamma[1]-t(a[[i-1]])%*%gamma[2:i])
}
```

```{r onestepplot,fig.cap="One-step ahead predictions\\label{fig:onestep}",echo=FALSE}
plot(x,col=2,type="l",xlim=c(1,100),main="",ylab=TeX("X_t,\\hat{X}_t,Z_t"),xlab="Time")
lines(xhat,col=4)
U <- xhat+xhatse
L <- xhat-xhatse
xx = c(time(U), rev(time(U))); yy = c(L, rev(U))
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2))
```


In figure \ref{fig:onestep} we plot the process \(X_t\) in red along with the one-step ahead predictions \(\hat{X_t}\) in blue and the corresponding standard error in gray. The prediction tends to be slightly closer to the x-axis than the process itself, especially for the more extreme values. Intuitively this makes sense since the process' extreme values occur around extreme values for the underlying white noise, which the prediction expects to be zero. 

``` {r forecasting,fig.cap="Forecasted values of the series\\label{fig:forecast}",echo=FALSE}
fore <- predict(arima,20)
ts.plot(c(x,fore$pred),col=1:2,xlim=c(420,520),ylab=TeX("X_t"))
U <- fore$pred+fore$se
L <- fore$pred-fore$se
xx = c(time(U), rev(time(U))); yy = c(L, rev(U))
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2))
lines(fore$pred, type="p", col=2)
```

Using the same linear predictor as above, we can forecast the next 20 values of \(X_t\) given \(X_1,\hdots,X_500\). This is done in figure \ref{fig:forecast}. As is to be expected, the forecast seems to gradually approach the series mean of zero with an expected error that approaches \(\sigma_X=\sqrt{\gamma(0)}\approx `r sprintf('%.3f',sqrt(gamma[1]))`\).

# Discussion


```{r modelchoice, echo=FALSE,eval=FALSE}
af<-autofit(x,p=0:7,q=0:7)
#aa<-auto.arima(x,d=0)
```

# Conclusion



# Appendix

What follows is a complete copy of the computer code used by R markdown to produce the report.

```{r appendix,eval=FALSE,echo=TRUE}

```

\begin{thebibliography}{9}
\bibitem{brockwelldavies}
  Brockwell, Peter J., Davis, Richard A.,
  \textit{Introduction to Time Series and Forecasting},
  2nd edition,
  2002.
\end{thebibliography}
