---
title: "State space and SARIMA model analysis"
subtitle: "TMA4285 Time series models - Exercise 8"
author: Anders Christiansen SÃ¸rby, Edvard Hove
#bibliography: mybiblio.bib
header-includes:
  - \usepackage{dsfont}
  - \usepackage{bm}
  - \DeclareMathOperator*{\E}{\mathrm{E}}
  - \DeclareMathOperator*{\Var}{\mathrm{Var}}
  - \DeclareMathOperator*{\Cov}{\mathrm{Cov}}
output: pdf_document
keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("itsmr")
#install.packages("forecast")
#install.packages("astsa")
#install.packages("tidyr")
library(forecast)
library(latex2exp)
library(itsmr)
library(astsa)
library(tidyr)
library(ggplot2)
```
\abstract{
}

# Introduction

In this project we will analyze a time series using two different models, namely the state space and the SARIMA models. First we will define them. 

## The State space model

A possibly multivariate state space model consists of two equations. First the observation equation 
\begin{equation}
Y_t = G_t X_t + W_t, \hspace{5em} t \in \mathbb{N}
\end{equation}
where $W_t \sim WN(0, \{R_t\})$. Secondly there is the state equation 
\begin{equation}
X_{t+1} = F_t X_t + V_t, \hspace{5em} t\in \mathbb{N}
\end{equation}
where $V_t \sim WN(0, \{Q_t\})$. 

## The SARIMA process

The seasonal ARIMA (SARIMA) process is a generalization of the ARIMA process can be defined as follows. Given two nonnegative integers $d$, and $D$, $\{X_t\}$ is a seasonal ARIMA$(p,d,q)\times(P,D,Q)_s$ with period $s$ if the differenced series $Y_t = (1-B)^d(1-B^s)^DX_t$ is a causal ARMA process defined by
\begin{equation}
\label{eq:SARIMA}
\phi(B)\Phi(B^s)Y_t = \theta(B)\Theta(B^s)Z_t,\hspace{5em} \{Z_t\}\sim WN(0,\sigma^2)
\end{equation}
where $\phi(z)= 1-\phi_1 z-\ldots- \phi_p z^p$, $\Phi(z) = 1- \Phi_1 z - \ldots - \Phi_p z^p$, $\theta(z) = 1+ \theta_1 z + \ldots + \theta_q z^q$ and $\Theta(z) = 1- \Theta_1 z - \ldots - \Theta_p z^p$. This can also be reformulated to using only one polynomial on each side. 

For causality for $Y_t$ to hold we need that $\phi(z)\neq 0$ and $\Phi(z)\neq 0$ for $|z| \leq 1$. The SARIMA allows for randomness in the seasonal pattern and is therefore more practical in applications than classical decomposition models. 


# Theory

## Model parameter estimation and uncertainty


<!--
REPEATED ESTIMATION ON SIMULATED DATA COMES HERE:
We need to confirm that a given process is indeed an $ARMA(2,1)$
-->
##Model choice

Given a collection of models, the Akaike information criterion estimates the quality of each model relative to the other models. 
As such it is a means for model selection.
The AIC is defined as 
\begin{equation}
AIC = 2k - 2\ln(\hat L),
\end{equation}
\noindent where $k$ is the number of estimated parameters in the model and $\hat{L}$ is the model's likelihood function's maximum value.
The model with the lower AIC should be selected, rewarding goodness of fit while punishing overfitting.
```{r read, echo=FALSE}
x = read.table("airpass.txt")
x <- x[,1]
l <- length(x)
```

# Data analysis

```{r TSMplot, echo=FALSE, fig.cap="\\label{fig:timeseries} The data set AirPassengers, showing monthly totals of international airline passengers."}
plot(seq(1,l),x,type="l", xlab="t", ylab=TeX("X_t"))
```

The realization of the process from `AirPassengers` can be seen in figure \ref{fig:timeseries}.
This process is clearly in need of some transformation.
First of all it has a clear upwards trend, secondly there is some seasonal behaviour, and thirdly the variance seems to be increasing.
A commonly used approach to solve the third problem is to consider the logarithm of the data.

```{r logTSMplot, echo=FALSE, fig.cap="\\label{fig:logts} The data set AirPassengers, after taking the logarithm of the data."}
lx <- log(x)
plot(seq(1,l),lx,type="l", xlab="t", ylab=TeX("log X_t"))
```

\noindent This is done in figure \ref{fig:logts}.
The transformation seems to have stabilized the variance fairly well.
Next we difference the logged data to remove the apparent linear trend.

```{r difflogTSMplot, echo=FALSE, fig.cap="\\label{fig:dlts} The data set AirPassengers, transformed by differencing the logged data."}
dlx<- diff(lx)
plot(seq(1,l-1),dlx,type="l", xlab="t", ylab=TeX("$\\nabla$ log X_t"))
```

\noindent The differenced and logged data can be seen in figure \ref{fig:dlts}.

```{r dlxACF, echo=FALSE, fig.cap="\\label{fig:ACFdlx} The sample autocorrelation function of the differenced logged data"}
Acf(dlx,main="")
```

\noindent After differencing the data still appears to have some seasonal effect.
It is natural to suspect a season of 12, since these are monthly data.
The ACF of $\nabla \log (X_t)$, as can be seen in figure \ref{fig:ACFdlx} has a clear peak at lag 12, thereby confirming these suspicions.
By applying a twelfth-order difference we attain what appears to be stationary data, as can be seen in figure \ref{fig:ddlxts}.
In terms of \eqref{eq:SARIMA}, we have are now considering a model where $d = 1, D = 1, s = 12$.


```{r ddlx, echo=FALSE,fig.cap="\\label{fig:ddlxts} The data from AirPassengers after transformation into an apparent stationary series."}
ddlx <- diff(dlx,12)
plot(seq(1,l-13),ddlx,type="l", xlab="t", ylab=TeX("$\\nabla_{12} \\nabla$ log X_t"))
```

```{r seasonal, echo=FALSE,fig.cap="\\label{fig:sacf} Sample ACF and PACF of $\\nabla_{12} \\nabla$ log X_t"}
test <- acf2(ddlx,50,main="")
```

To determine the seasonal components, as in \eqref{eq:SARIMA}, we consider the ACF and PACF of the transformed data.
These are shown in figure \ref{fig:sacf}.
At the seasonal lags, i.e. the lags which are multiples of $s$, the ACF cuts off after one season.
This suggests that a seasonal component on the form of an SMA(1), that is $P = 0, Q = 1$.
For the smaller lags, both the ACF and PACF gradually decrease in a way that suggests the within seasons part could be on the form of an ARMA(1,1), that is $p = 1, q = 1$. 
By using these estimates for $p,P,q$ and $Q$ we can now estimate the coefficients in the polynomials $\phi, \theta$ and $\Theta$.
Using the R function `arima` gives

```{r sarima1}
ordr <- c(1,1,1)
seasonal <- list(order= c(0,1,1), period = 12)
sarima1 <- arima(lx,,order=ordr, seasonal=seasonal)
sarima1
```

Since the `ar1` coefficient is less than one standard deviation away from 0, it is conceivable that this model is overfitting the non-seasonal component.
In order to get better estimates for the order of the polynomials, we use the R function `auto.arima` which uses the AICc to select the best from a range of models.
This gives the following model:

```{r autoarima, include = FALSE}
slx <- ts(lx,frequency=12,start=c(1949,1))
auto.arima(slx,d = NA, D = NA, max.p=4, max.q=4, max.P=2, max.Q=2,
                     max.order=20, max.d=2, max.D=1, start.p=1, start.q=1,
                     start.P=1, start.Q=1, stationary=FALSE, seasonal=TRUE,
                     trace=TRUE)
ord <- c(0,1,1)
seas <- list(order=c(0,1,1),period=12)
sarima <- arima(slx,order=ord,seasonal=seas)
```
```{r sarima2, echo=FALSE}
sarima
```
Hence the best estimates for $p, d, q, P, D$ and $Q$, assuming $s=12$, results in an ARIMA(0,1,1)\texttimes$(0,1,1)_{12}$ model with coefficients as in the output above.
By simulating from an ARIMA(0,1,1)\texttimes$(0,1,1)_{12}$ model with these coefficients we can get an empirical estimation of their distributions.
This also allows us to estimate the uncertainty of $\sigma^2$, which is not given by the printout above.

```{r simulations, eval=FALSE,include=FALSE}
n_sim <- 500
ts_sims_arima <- NULL
ts_sims <-  matrix(data = 0, ncol=144, nrow=n_sim)
ts_sims_param <- matrix(data = 0, ncol=3, nrow=n_sim, dimnames = list(NULL, c( "ma1","sma1","sigma^2")))

for (i in 1:n_sim) {
  ts_sim <- simulate(sarima)
  ts_sims[i,] <- ts_sim
  ts <- arima(ts_sim, order = ord, seasonal = seas)
  ts_sims_arima <- c(ts_sims_arima, ts)
  ma1_sim <- ts$coef[1]
  sma1_sim <- ts$coef[2]
  sigma2_sim <- ts$sigma2
  ts_sims_param[i,1] <- ma1_sim
  ts_sims_param[i,2] <- sma1_sim
  ts_sims_param[i,3] <- sigma2_sim
}

saveRDS(ts_sims_param, "ts_sims_param.txt")
```
```{r printsimparam,echo=FALSE}
ts_sims_param <- as.data.frame(readRDS("ts_sims_param.txt"))
n_sim <- length(ts_sims_param[,1])
mean_param <- colSums(ts_sims_param)/n_sim
covariance_param <- var(ts_sims_param)

print("mean parameter values")
print(mean_param)

print("parameter sample covariance")
print(covariance_param)

print("parameter sample variance")
print(sqrt(diag(covariance_param)))
```

```{r simulationshist, echo = FALSE, fig.cap="\\label{fig:arimasimparam} Estimated coefficients from 500 realizations of the ARIMA(0,1,1)\\texttimes$(0,1,1)_{12}$ process, given the estimated coefficients from the original series."}
ggplot(gather(ts_sims_param), aes(value)) + 
    geom_histogram(bins = 10,col = "black",fill = "cyan3") + 
    facet_wrap(~key, scales = 'free_x')
```

## Model prediction

Using the R function `sarima.for` we can forecast the logged data for the next year.
The result is shown in figure \ref{fig:forecast}

```{r forecast, fig.cap="\\label{fig:forecast} The forecasted values of the logged air passenger data for the next twelve months in red, with standard errors in grey"}
forecast <- sarima.for(slx,12,0,1,1,0,1,1,12)
```

Since the process $Y_t$ from equation \eqref{eq:SARIMA} is a weakly stationary ARMA process, we can create its best linear predictors as we did previously.
That is 

\begin{equation}
P_n Y_{n+h} = \sum_{i=1}^n a_i Y_{n+h-i}
\end{equation}

\noindent where $a_i$ is such that

\begin{equation}
\Gamma_n \bm{a}_n = \gamma_n(h)
\end{equation}
<!--
\noindent where 
\begin{equation}
\gamma_n = (\gamma(h),\gamma(h+1),\hdots,\gamma(h+n-1))',
\end{equation}

\noindent and
\begin{equation}
\Gamma_n = [\Cov(Y_i,Y_j)]_{i,j=1}^n.
\end{equation}
-->
\noindent As before, the expected prediction error is zero, and the expected mean square prediction error is given by

\begin{equation}
\E[(Y_{n+h}-P_nY_{n+h})^2] = \gamma(0) - \bm{a}'\gamma_n(h).
\end{equation}

In figure \ref{fig:onestepy} we plot the process \(Y_t\) in red along with the one-step ahead predictions \(\hat{Y_t}\) in blue and the corresponding standard error in gray. 

```{r onestep, echo=FALSE}
arma_y <- arima(ddlx,order = c(0,0,13),fixed=c(sarima$coef[1],0,0,0,0,0,0,0,0,0,0,sarima$coef[2],sarima$coef[1]*sarima$coef[2],0))
arma_y_components <- list(phi=0,theta=c(arma_y$coef[1:13]),sigma2=arma_y$sigma2)
ls <- l - 13
gamma<-aacvf(arma_y_components,130)
Gamma <- matrix(ncol=ls,nrow=ls)
for (i in 1:ls){
  for (j in 1:ls){
    Gamma[i,j] <- gamma[abs(i-j)+1]
  }
}

a <- vector("list",ls-1)
for(i in 1:(ls-1)){
  a[[i]] = solve(Gamma[1:i,1:i])%*%gamma[2:(i+1)]
}

yhat <-rep(0,ls)
yhatse <- rep(gamma[1],ls)
for (i in 2:ls){
  yhat[i] <- t(a[[i-1]])%*%ddlx[(i-1):1]
  yhatse[i] <- sqrt(gamma[1]-t(a[[i-1]])%*%gamma[2:i])
}
```

```{r onestepploty,fig.cap="One-step ahead predictions of the transformed data in blue with standard errors in gray\\label{fig:onestepy}",echo=FALSE}
plot(ddlx,col=2,type="l",xlim=c(1,100),main="",ylab=TeX("Y_t,\\hat{Y}_t"),xlab="Time")
lines(yhat,col=4)
U <- yhat+yhatse
L <- yhat-yhatse
xx = c(time(U), rev(time(U))); yy = c(L, rev(U))
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2))
```

The one-step ahead predictions for the model can then be found simply by transforming the data back.
First we reverse the differencing, by considering the first 13 data points as initial conditions and using the following equation.

\begin{align}
X_t =& Y_t + \gamma_1 X_{t-1} + \dots + \gamma_{p_{\gamma}} X_{t-p_{\gamma}} \\
=& Y_t + X_{t-1} + X_{t-12} - X_{t-13}
\end{align}

By inserting $\hat{Y_t}$ into the equations above, we get $\hat{X_t}$ which is the one step prediction for the logged data. 
The one step prediction for the real data is then found by exponentiation.
This will of course change the standard error.
In figure \ref{fig:onestepx} the onestep prediction for the ARIMA(0,1,1)\texttimes$(0,1,1)_{12}$ model is plotted in blue, with the observed data in red.
The gray outlines are the transformed standard errors for the logged data, not the \textit{real} standard errors.
```{r onestepx}
lxhat <- rep(0,13)
for (i in 14:l){
  lxhat <- c(lxhat, yhat[i-13]+ lx[i-1]+lx[i-12]-lx[i-13])
}
lxhatse <- c(rep(0,13),yhatse)
xhat <- exp(lxhat)
```

```{r onestepplotx,fig.cap="One-step ahead predictions from the ARIMA model in blue with standard errors (transformed from the logged data) in gray.\\label{fig:onestepx}",echo=FALSE}
plot(x,col=2,type="l",xlim=c(1,144),main="",ylab=TeX("X_t,\\hat{X}_t"),xlab="Time")
lines(xhat,col=4)
U <- exp(lxhat + lxhatse)
L <- exp(lxhat - lxhatse)
xx = c(time(U), rev(time(U))); yy = c(L, rev(U))
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2))
```

```{r sarimaresiduals,echo=FALSE,fig.cap="\\label{fig:sarimares}Plot of the residuals"}
plot(sarima$residuals[14:144],type="p",ylab="Residuals")
```

```{r sarimaresqq,echo=FALSE,fig.cap="\\label{fig:sarimaresqq}Q-Q plot of the standardized residuals"}
qqnorm(sarima$residuals[14:144]/sd(sarima$residuals[14:144]),main="")
qqline(sarima$residuals[14:144]/sd(sarima$residuals[14:144]),col=2)
```

\noindent Based on the residual diagnostics in figurs \ref{fig:sarimares} and \ref{fig:sarimaresqq} the model seems to fit well.
The standardized residuals seem uncorrelated and correspond closely with the standard normal distribution. 

# Discussion


```{r modelchoice, echo=FALSE,eval=FALSE}
af<-autofit(x,p=0:7,q=0:7)
#aa<-auto.arima(x,d=0)
```

# Conclusion



# Appendix

What follows is a complete copy of the computer code used by R markdown to produce the report.

```{r appendix,eval=FALSE,echo=TRUE}

```

\begin{thebibliography}{9}
\bibitem{brockwelldavies}
  Brockwell, Peter J., Davis, Richard A.,
  \textit{Introduction to Time Series and Forecasting},
  2nd edition,
  2002.
\end{thebibliography}
