---
title: "State space and SARIMA model analysis"
subtitle: "TMA4285 Time series models - Exercise 8"
author: Anders Christiansen SÃ¸rby, Edvard Hove
#bibliography: mybiblio.bib
header-includes:
  - \usepackage{dsfont}
  - \usepackage{bm}
  - \DeclareMathOperator*{\E}{\mathrm{E}}
  - \DeclareMathOperator*{\Var}{\mathrm{Var}}
  - \DeclareMathOperator*{\Cov}{\mathrm{Cov}}
output: pdf_document
keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("itsmr")
#install.packages("forecast")
#install.packages("astsa")
#install.packages("tidyr")
#install.packages("dse")
#install.packages("stsm")
#install.packages("KFKSDS")
library(forecast)
library(latex2exp)
library(itsmr)
library(astsa)
library(tidyr)
library(ggplot2)
library(dse)
library(stsm)
library(KFKSDS)
```
\abstract{
}

# Introduction

In this project we will analyze a time series using two different models, namely the state space and the SARIMA models. First we will define them. 

## The State space model

A possibly multivariate state space model consists of two equations. First the observation equation 
\begin{equation}
Y_t = G_t X_t + W_t, \hspace{5em} t \in \mathbb{N} \label{ssobs}
\end{equation}
where $W_t \sim WN(0, \{R_t\})$. Secondly there is the state equation 
\begin{equation}
X_{t+1} = F_t X_t + V_t, \hspace{5em} t\in \mathbb{N} \label{ssstate}
\end{equation}
where $V_t \sim WN(0, \{Q_t\})$. In this analysis we are going to use the special case The Basic Structural Model which we will define later. 

## The SARIMA process

The seasonal ARIMA (SARIMA) process is a generalization of the ARIMA process and can be defined as follows. Given two nonnegative integers $d$, and $D$, $\{X_t\}$ is a seasonal ARIMA$(p,d,q)\times(P,D,Q)_s$ with period $s$ if the differenced series $Y_t = (1-B)^d(1-B^s)^DX_t$ is a causal ARMA process defined by
\begin{equation}
\label{eq:SARIMA}
\phi(B)\Phi(B^s)Y_t = \theta(B)\Theta(B^s)Z_t,\hspace{5em} \{Z_t\}\sim WN(0,\sigma^2)
\end{equation}
where $\phi(z)= 1-\phi_1 z-\ldots- \phi_p z^p$, $\Phi(z) = 1- \Phi_1 z - \ldots - \Phi_p z^p$, $\theta(z) = 1+ \theta_1 z + \ldots + \theta_q z^q$ and $\Theta(z) = 1- \Theta_1 z - \ldots - \Theta_p z^p$. This can also be reformulated to using only one polynomial on each side. 

For causality for $Y_t$ to hold we need that $\phi(z)\neq 0$ and $\Phi(z)\neq 0$ for $|z| \leq 1$. The SARIMA allows for randomness in the seasonal pattern and is therefore more practical in applications than classical decomposition models. 

# Theory

##Forecasting SARIMA Processes

Prediction for SARIMA processes can be done by applying $P_n$ to both sides of the equation

\begin{equation}
(1-B)^d(1-B)^D X_t = \gamma(B) X_t = Y_t.
\end{equation}

\noindent The ARMA process $Y_t$ can then be predicted in the same way as previously.
If we consider the first $d+Ds$ observations of $X_t$ to be intial conditions that are uncorrelated with $\{Y_t, t\geq 1\}$ we immediately get

\begin{equation}
\label{eq:SARIMAfor}
P_n X_{n+h} = P_n Y_{n+h} + \sum_{j=1}^{d+Ds}\gamma_j P_n X_{n+h-j}.
\end{equation}

\noindent The predictors $P_n X_{n+h}$ can then be computed recursively for $h=1,2,\dots$, since $P_n X_{n+1-j} = X_{n+1-j}$ by definition.
The prediction mean squared error is given by

\begin{equation}
\E(X_{n+h}- P_n X_{n+h})^2 = \sum_{j=0}^{h-1}\left(\sum_{r=0}^j \chi_r \theta_{n+h-r-1,j-r} \right)^2 v_{n+h-j-1},
\end{equation}

\noindent where $\theta_{nj}$ and $v_n$ are from the innovations algorithm applied on the differenced series $\{Y_t\}$.

## Model parameter estimation and uncertainty

For the state space model we can assume that the initial state, ${X}_1$, observation $Y_0$, and the noice vectors $W_t$ and $V_t$ are jointly Gaussian. This simplifies paramter estimation greatly. We are going to need the Kalman prediction for state space models to estimate parameters and do prediction. For state space models the best linear predictors $\hat{X}_t:=\hat{E}[X_t|Y_{t-1},\ldots, Y_0]$ and corresponding error covariance matrices $\Omega_t:=E[({X}_t-\hat{{X}}_t)(X_t-\hat{{X}}_t)^T]$ are uniquely determined by the initial conditions $\hat{{X}}_1$ and $\Omega_1$ and the Kalman recursions 
$$
\hat{X}_{t+1} = F_{t} \hat{X}_t + \Theta_t \Delta_t^{-1}(Y_t-G_t \hat{X}_t),
$$
$$
\Omega_{t+1} = F_t \Omega_t F_t^T + Q_t-\Theta_t\Delta_t^{-1}\Theta_t^T
$$
\noindent where $\Delta_t=G_t\Omega_tG_t^T + R_t$ and $\Theta_t=F_t\Omega_tG_t^T$. This gives us a way to estimate the state given some observations which we will use in the maximum likelihood.  


\noindent We can now define an explicit conditional density
\begin{equation}
f_t({Y}_t | {Y}_{t-1}, \ldots,{Y}_0) = (2\pi)^{-w/2}(det\Delta_t)^{-1/2}\exp\left(-\frac{1}{2}I_t^T\Delta^{-1}I_t\right),
\end{equation}
\noindent and a likelihood function
\begin{equation}
L(\theta ; Y_{t-1}, \ldots, {Y}_0) =  (2\pi)^{-nw/2} \left(\prod_{j=1}^n\det\Delta_j\right)^{-1/2} \exp\left(-\frac{1}{2} \sum_{j=0}^n I_t^T \Delta_j^{-1} I_t \right),
\end{equation}
\noindent where $I_t$ are the one-step prediction errors from the Kalman Recursions and $\Delta_t$ are their covariances. 
We can get the ML estimate for $\theta$ by maximizing this function.

For structural models, where the matrices $F_t$ and $G_t$ are assumed to be the time-invariant matrices $F$ and $G$, estimation is made easier by assuming that $X_1$ is equal to a deterministic but unknown parameter $\mathbf{\mu}$.
Then $\hat{X}_1 = \mathbf{\mu}$, which means that $\Omega_1 = 0$.
The parameters of the model are then $\mathbf{\mu}, Q,$ and $\sigma_w^2$.
Maximization of the likelihood can then be simplified by instead considering the \textit{reduced likelihood}, that is $L(\hat{\mathbf{\mu}}(Q), Q, \hat{\sigma_w^2}(Q))$.

##Model choice

Given a collection of models, the Akaike information criterion estimates the quality of each model relative to the other models. 
As such it is a means for model selection.
The AIC is defined as 
\begin{equation}
AIC = 2k - 2\ln(\hat L),
\end{equation}
\noindent where $k$ is the number of estimated parameters in the model and $\hat{L}$ is the model's likelihood function's maximum value.
The model with the lower AIC should be selected, rewarding goodness of fit while punishing overfitting.
```{r read, echo=FALSE}
x = read.table("airpass.txt")
x <- x[,1]
l <- length(x)
```

###The Basic Structural Model

A possible simple choice of model for the state space model is The Basic Structural Model (BSM). We have state space model like in \eqref{ssobs} and \eqref{ssstate} and the following definition for $F$, $Q$ and $G$,
\[
F=
  \begin{bmatrix}
    1 & 1 & 0  & 0 & \ldots & 0 & 0 \\
    0 & 1 & 0 & 0  & \ldots & 0 & 0 \\
    0 & 0 & -1 & 0 &\ldots & -1 & -1  \\
    0 & 0 & 1 & 0 & \ldots & 0  & 0 \\ 
    0 & 0 & 0 & 1 & \ldots & 0  & 0\\ 
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\ 
    0 & 0 & 0 & 0 & \ldots & 1  & 0\\ 
  \end{bmatrix}
\]
\[
G = 
  \begin{bmatrix}
    1 & 0 & 1  & 0 & \ldots & 0 & 0 
  \end{bmatrix}
\]

\[
Q=
  \begin{bmatrix}
    \sigma_1^2 & 1 & 0  & 0 & \ldots & 0 & 0 \\
    0 & \sigma_2^2 & 0 & 0  & \ldots & 0 & 0 \\
    0 & 0 & \sigma_3^2 & 0 &\ldots & 0 & 0  \\
    0 & 0 & 0 & 0 & \ldots & 0  & 0 \\ 
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\ 
    0 & 0 & 0 & 0 & \ldots & 0  & 0\\ 
  \end{bmatrix}
\]

The state vector $X_t$ is a 13-dimensional and the matricies have the dimension suitable for this form. This means that the there are in total 5 parameters that we need to estimate namely $\sigma_1^2$, $\sigma_2^2$, $\sigma_3^2$, $\sigma_w^2 = W_t$ and $\mu=X_1$. We will use maximum likelihood to estimate this. 


# Data analysis

```{r TSMplot, echo=FALSE, fig.cap="\\label{fig:timeseries} The data set AirPassengers, showing monthly totals of international airline passengers."}
plot(seq(1,l),x,type="l", xlab="t", ylab=TeX("X_t"))
```

The realization of the process from `AirPassengers` can be seen in figure \ref{fig:timeseries}.
This process is clearly in need of some transformation.
First of all it has a clear upwards trend, secondly there is some seasonal behaviour, and thirdly the variance seems to be increasing.
A commonly used approach to solve the third problem is to consider the logarithm of the data.

```{r logTSMplot, echo=FALSE, fig.cap="\\label{fig:logts} The data set AirPassengers, after taking the logarithm of the data."}
lx <- log(x)
plot(seq(1,l),lx,type="l", xlab="t", ylab=TeX("log X_t"))
```

\noindent This is done in figure \ref{fig:logts}.
The transformation seems to have stabilized the variance fairly well.
Next we difference the logged data to remove the apparent linear trend.

```{r difflogTSMplot, echo=FALSE, fig.cap="\\label{fig:dlts} The data set AirPassengers, transformed by differencing the logged data."}
dlx<- diff(lx)
plot(seq(1,l-1),dlx,type="l", xlab="t", ylab=TeX("$\\nabla$ log X_t"))
```

\noindent The differenced and logged data can be seen in figure \ref{fig:dlts}.

```{r dlxACF, echo=FALSE, fig.cap="\\label{fig:ACFdlx} The sample autocorrelation function of the differenced logged data"}
Acf(dlx,main="")
```

\noindent After differencing the data still appears to have some seasonal effect.
It is natural to suspect a season of 12, since these are monthly data.
The ACF of $\nabla \log (X_t)$, as can be seen in figure \ref{fig:ACFdlx} has a clear peak at lag 12, thereby confirming these suspicions.
By applying a twelfth-order difference we attain what appears to be stationary data, as can be seen in figure \ref{fig:ddlxts}.
In terms of \eqref{eq:SARIMA}, we have are now considering a model where $d = 1, D = 1, s = 12$.


```{r ddlx, echo=FALSE,fig.cap="\\label{fig:ddlxts} The data from AirPassengers after transformation into an apparent stationary series."}
ddlx <- diff(dlx,12)
plot(seq(1,l-13),ddlx,type="l", xlab="t", ylab=TeX("$\\nabla_{12} \\nabla$ log X_t"))
```

```{r seasonal, echo=FALSE,fig.cap="\\label{fig:sacf} Sample ACF and PACF of $\\nabla_{12} \\nabla$ log X_t"}
test <- acf2(ddlx,50,main="")
```

To determine the seasonal components, as in \eqref{eq:SARIMA}, we consider the ACF and PACF of the transformed data.
These are shown in figure \ref{fig:sacf}.
At the seasonal lags, i.e. the lags which are multiples of $s$, the ACF cuts off after one season.
This suggests that a seasonal component on the form of an SMA(1), that is $P = 0, Q = 1$.
For the smaller lags, both the ACF and PACF gradually decrease in a way that suggests the within seasons part could be on the form of an ARMA(1,1), that is $p = 1, q = 1$. 
By using these estimates for $p,P,q$ and $Q$ we can now estimate the coefficients in the polynomials $\phi, \theta$ and $\Theta$.
Using the R function `arima` gives

```{r sarima1}
ordr <- c(1,1,1)
seasonal <- list(order= c(0,1,1), period = 12)
sarima1 <- arima(lx,order=ordr, seasonal=seasonal)
sarima1
```

Since the `ar1` coefficient is less than one standard deviation away from 0, it is conceivable that this model is overfitting the non-seasonal component.
In order to get better estimates for the order of the polynomials, we use the R function `auto.arima` which uses the AICc to select the best from a range of models.
This gives the following model:

```{r autoarima, include = FALSE}
slx <- ts(lx,frequency=12,start=c(1949,1))
auto.arima(slx,d = NA, D = NA, max.p=4, max.q=4, max.P=2, max.Q=2,
                     max.order=20, max.d=2, max.D=1, start.p=1, start.q=1,
                     start.P=1, start.Q=1, stationary=FALSE, seasonal=TRUE,
                     trace=TRUE)
ord <- c(0,1,1)
seas <- list(order=c(0,1,1),period=12)
sarima <- arima(slx,order=ord,seasonal=seas)
```

```{r sarima2, echo=FALSE}
sarima
```

Hence the best estimates for $p, d, q, P, D$ and $Q$, assuming $s=12$, results in an ARIMA(0,1,1)\texttimes$(0,1,1)_{12}$ model with coefficients as in the output above.
By simulating from an ARIMA(0,1,1)\texttimes$(0,1,1)_{12}$ model with these coefficients we can get an empirical estimation of their distributions.
This also allows us to estimate the uncertainty of $\sigma^2$, which is not given by the printout above.

```{r simulations, eval=FALSE,include=FALSE}
n_sim <- 500
ts_sims_arima <- NULL
ts_sims <-  matrix(data = 0, ncol=144, nrow=n_sim)
ts_sims_param <- matrix(data = 0, ncol=3, nrow=n_sim, dimnames = list(NULL, c( "ma1","sma1","sigma^2")))

for (i in 1:n_sim) {
  ts_sim <- simulate(sarima)
  ts_sims[i,] <- ts_sim
  ts <- arima(ts_sim, order = ord, seasonal = seas)
  ts_sims_arima <- c(ts_sims_arima, ts)
  ma1_sim <- ts$coef[1]
  sma1_sim <- ts$coef[2]
  sigma2_sim <- ts$sigma2
  ts_sims_param[i,1] <- ma1_sim
  ts_sims_param[i,2] <- sma1_sim
  ts_sims_param[i,3] <- sigma2_sim
}

saveRDS(ts_sims_param, "ts_sims_param.txt")
```
```{r printsimparam,echo=FALSE}
ts_sims_param <- as.data.frame(readRDS("ts_sims_param.txt"))
n_sim <- length(ts_sims_param[,1])
mean_param <- colSums(ts_sims_param)/n_sim
covariance_param <- var(ts_sims_param)

print("mean parameter values")
print(mean_param)

print("parameter sample covariance")
print(covariance_param)

print("parameter sample variance")
print(sqrt(diag(covariance_param)))
```

```{r simulationshist, echo = FALSE, fig.cap="\\label{fig:arimasimparam} Estimated coefficients from 500 realizations of the ARIMA(0,1,1)\\texttimes$(0,1,1)_{12}$ process, given the estimated coefficients from the original series."}
ggplot(gather(ts_sims_param), aes(value)) + 
    geom_histogram(bins = 10,col = "black",fill = "cyan3") + 
    facet_wrap(~key, scales = 'free_x')
```

## Model prediction

###State space

We are using the Basic Structural model, as presented earlier, given by the r function StructTS to fit our data. This was significantly easier than other approaches. It is important to choose good initial values for the method to acually find a good fit for the model.

<!--
```{r ssest}
num <- l
Y <- x # Less confusing
G <- t(rep(0,13))
G[1] <- 1
G[3] <- 1
F <- diag(0, ncol = 13, nrow = 13)
F[1,1] <- F[1,2] <- F[2,2] <- 1
F[4:13, 3:12] <- diag(1,10)
F[3,3:13] <- -1
#initial values
Sigma0 <- diag(0.04, 13)
z0 <- runif(13)
sigma_w <- 1
#model <- SS(F=F, H=G, z0=z0, Q=matrix(0,13),R=sigma_w)

SSML <- function(param) {
  cQ <- matrix(0, 13, 13)
  
  cQ[1,1] <- param[1]
  cQ[2,2] <- param[2]
  cQ[3,3] <- param[3]
  
  cR <- param[4]
  mu <- param[5:17]
  
  kf <- Kfilter0(num, Y, G, mu, Sigma0, F, cQ, cR)
  return(kf$like)
}
mu0 <- c(140, 2, -30, -30, -50, -17, 20, 50, 50, 30, 0, 0, 0)
init.par <- t(c(170, 0, 11, 0.01, mu0))
#est <- optim(init.par, SSML, NULL, method = "BFGS", hessian = TRUE, control = list(trace=1, REPORT=1))
#SE= sqrt(diag(solve(est$hessian)))
```
-->
```{r tsstruct,echo=FALSE}
xs <- ts(log(x),frequency=12)
fit1 <- StructTS(xs, type = "BSM")
fit1$coef
fit1$model$a
```

```{r stsm,warning=FALSE,echo=FALSE}
m <- stsm.model(model = "BSM", y = xs, transPars = "StructTS")
fit2 <- stsmFit(m, stsm.method = "maxlik.td.optim", method = "L-BFGS-B", 
  KF.args = list(P0cov = TRUE))
fit2.comps <- tsSmooth(fit2, P0cov = FALSE)$states
plot(fit2.comps, main = "")
```


```{r predictionbsm,echo=FALSE,fig.cap="\\label{onestepss} The logged data in red along with its one-step predictions according to the state space model."}
fitted <- fitted(fit1)
yhatss <- fitted[,1]+fitted[,3]
plot(seq(1:144),xs,type="l",col="red",ylab=TeX("Y_t,\\hat{Y}_t"),xlab="Time")
lines(seq(1:144),c(yhatss[1],yhatss[1:143]),col="blue")
```


```{r forecastbsm,echo=FALSE}
require("KFKSDS")
m2 <- set.pars(m, pmax(fit2$par, .Machine$double.eps))
ss <- char2numeric(m2)
pred <- predict(ss, xs, n.ahead = 12)

par(mfrow = c(3,1), mar = c(3,3,3,3))
# observed series
plot(cbind(xs, pred$pred), type = "n", plot.type = "single", ylab = "") #, ylim = c(8283372, 19365461))
lines(xs)
polygon(c(time(pred$pred), rev(time(pred$pred))), c(pred$pred + 2 * pred$se, rev(pred$pred)), col = "gray85", border = NA)
polygon(c(time(pred$pred), rev(time(pred$pred))), c(pred$pred - 2 * pred$se, rev(pred$pred)), col = " gray85", border = NA)
lines(pred$pred, col = "blue", lwd = 1.5)
mtext(text = "forecasts of the observed series", side = 3, adj = 0)
# level component
plot(cbind(xs, pred$a[,1]), type = "n", plot.type = "single", ylab = "") #, ylim = c(8283372, 19365461)) 
lines(xs)
polygon(c(time(pred$a[,1]), rev(time(pred$a[,1]))), c(pred$a[,1] + 2 * sqrt(pred$P[,1]), rev(pred$a[,1])), col = "gray85", border = NA)
polygon(c(time(pred$a[,1]), rev(time(pred$a[,1]))), c(pred$a[,1] - 2 * sqrt(pred$P[,1]), rev(pred$a[,1])), col = " gray85", border = NA)
lines(pred$a[,1], col = "blue", lwd = 1.5)
mtext(text = "forecasts of the level component", side = 3, adj = 0)
# seasonal component
plot(cbind(fit2.comps[,3], pred$a[,3]), type = "n", plot.type = "single", ylab = "") #, ylim = c(-3889253, 3801590))
lines(fit2.comps[,3])
polygon(c(time(pred$a[,3]), rev(time(pred$a[,3]))), c(pred$a[,3] + 2 * sqrt(pred$P[,3]), rev(pred$a[,3])), col = "gray85", border = NA)
polygon(c(time(pred$a[,3]), rev(time(pred$a[,3]))), c(pred$a[,3] - 2 * sqrt(pred$P[,3]), rev(pred$a[,3])), col = " gray85", border = NA)
lines(pred$a[,3], col = "blue", lwd = 1.5)
mtext(text = "forecasts of the seasonal component", side = 3, adj = 0)
```


```{r plotSS, fig.cap="\\label{fig:structfit} Residuals of the structural model"}
tsdiag(fit1)
```

```{r plotSS2, echi=FALSE, fig.cap="\\label{fig:structfit2} Fitted values for the state, according to the structural model"}
plot(tsSmooth(fit1),main="")
```

We can see the model fit in figure \ref{fig:structfit} which seems to be not so good. There seems to be a dependency

### SARIMA

Since the process $Y_t$ from equation \eqref{eq:SARIMA} is a weakly stationary ARMA process, we can create its best linear predictors as we did previously.
That is 

\begin{equation}
P_n Y_{n+h} = \sum_{i=1}^n a_i Y_{n+h-i}
\end{equation}

\noindent where $a_i$ is such that

\begin{equation}
\Gamma_n \bm{a}_n = \gamma_n(h)
\end{equation}
<!--
\noindent where 
\begin{equation}
\gamma_n = (\gamma(h),\gamma(h+1),\hdots,\gamma(h+n-1))',
\end{equation}

\noindent and
\begin{equation}
\Gamma_n = [\Cov(Y_i,Y_j)]_{i,j=1}^n.
\end{equation}
-->
\noindent As before, the expected prediction error is zero, and the expected mean square prediction error is given by

\begin{equation}
\E[(Y_{n+h}-P_nY_{n+h})^2] = \gamma(0) - \bm{a}'\gamma_n(h).
\end{equation}

In figure \ref{fig:onestepy} we plot the transformed and differenced process \(Y_t\) in red along with its one-step ahead predictions \(\hat{Y_t}\) in blue and the corresponding standard error in gray. 


```{r onestep, echo=FALSE}
arma_y <- arima(ddlx,order = c(0,0,13),fixed=c(sarima$coef[1],0,0,0,0,0,0,0,0,0,0,sarima$coef[2],sarima$coef[1]*sarima$coef[2],0))
arma_y_components <- list(phi=0,theta=c(arma_y$coef[1:13]),sigma2=arma_y$sigma2)
ls <- l - 13
gamma<-aacvf(arma_y_components,130)
Gamma <- matrix(ncol=ls,nrow=ls)
for (i in 1:ls){
  for (j in 1:ls){
    Gamma[i,j] <- gamma[abs(i-j)+1]
  }
}

a <- vector("list",ls-1)
for(i in 1:(ls-1)){
  a[[i]] = solve(Gamma[1:i,1:i])%*%gamma[2:(i+1)]
}

yhat <-rep(0,ls)
yhatse <- rep(gamma[1],ls)
for (i in 2:ls){
  yhat[i] <- t(a[[i-1]])%*%ddlx[(i-1):1]
  yhatse[i] <- sqrt(gamma[1]-t(a[[i-1]])%*%gamma[2:i])
}
```


```{r onestepploty,fig.cap="One-step ahead predictions of the SARIMA models' transformed data in blue with standard errors in gray\\label{fig:onestepy}",echo=FALSE}
plot(ddlx,col=2,type="l",xlim=c(1,100),main="",ylab=TeX("Y_t,\\hat{Y}_t"),xlab="Time")
lines(yhat,col=4)
U <- yhat+yhatse
L <- yhat-yhatse
xx = c(time(U), rev(time(U))); yy = c(L, rev(U))
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2))
```

The one-step ahead predictions for the model can then be found using equation \eqref{eq:SARIMAfor}.
Note that in our case

\begin{align}
X_t =& Y_t + \gamma_1 X_{t-1} + \dots + \gamma_{p_{\gamma}} X_{t-p_{\gamma}} \\
=& Y_t + X_{t-1} + X_{t-12} - X_{t-13}.
\end{align}

By inserting $\hat{Y_t}$ into the equations above, we get $\hat{X_t}$ which is the one step prediction for the logged data. 
The one step prediction for the real data is then found by exponentiation.
This will of course change the standard error.
In figure \ref{fig:onestepx} the onestep prediction for the ARIMA(0,1,1)\texttimes$(0,1,1)_{12}$ model is plotted in blue, with the observed data in red.
The gray outlines are the transformed standard errors for the logged data, not the \textit{real} standard errors.
```{r onestepx,include=FALSE}
lxhat <- rep(0,13)
for (i in 14:l){
  lxhat <- c(lxhat, yhat[i-13]+ lx[i-1]+lx[i-12]-lx[i-13])
}
lxhatse <- c(rep(0,13),yhatse)
xhat <- exp(lxhat)
```

```{r onestepplotx,fig.cap="One-step ahead predictions from the SARIMA model in blue with standard errors (transformed from the logged data) in gray.\\label{fig:onestepx}",echo=FALSE}
plot(x,col=2,type="l",xlim=c(1,144),main="",ylab=TeX("X_t,\\hat{X}_t"),xlab="Time")
lines(xhat,col=4)
U <- exp(lxhat + lxhatse)
L <- exp(lxhat - lxhatse)
xx = c(time(U), rev(time(U))); yy = c(L, rev(U))
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2))
```

```{r forecast2, fig.cap="\\label{fig:forecast} The SARIMA models' forecasted values of the logged air passenger data for the next twelve months in red, with standard errors in grey"}
forecast <- sarima.for(slx,12,0,1,1,0,1,1,12)
```

Using the R function `sarima.for` we can forecast the logged data for the next year.
The result is shown in figure \ref{fig:forecast}


```{r sarimaresiduals,echo=FALSE,fig.cap="\\label{fig:sarimares}Plot of the SARIMA models' residuals"}
plot(sarima$residuals[14:144],type="p",ylab="Residuals")
```

```{r sarimaresqq,echo=FALSE,fig.cap="\\label{fig:sarimaresqq}Q-Q plot of the SARIMA models' standardized residuals"}
qqnorm(sarima$residuals[14:144]/sd(sarima$residuals[14:144]),main="")
qqline(sarima$residuals[14:144]/sd(sarima$residuals[14:144]),col=2)
```

\noindent Based on the residual diagnostics in figurs \ref{fig:sarimares} and \ref{fig:sarimaresqq} the model seems to fit well.
The standardized residuals seem uncorrelated and correspond closely with the standard normal distribution. 

# Discussion

The State Space model is a more general model than SARIMA so it would possibly be able to incorporate more interesting features. However the SARIMA is performing much better than the State Space and is much easier to analyze. 


```{r modelchoice, echo=FALSE,eval=FALSE}
af<-autofit(x,p=0:7,q=0:7)
#aa<-auto.arima(x,d=0)
```

# Conclusion



# Appendix

What follows is a complete copy of the computer code used by R markdown to produce the report.

```{r appendix,eval=FALSE,echo=TRUE}

```

\begin{thebibliography}{9}
\bibitem{brockwelldavies}
  Brockwell, Peter J., Davis, Richard A.,
  \textit{Introduction to Time Series and Forecasting},
  2nd edition,
  2002.
\end{thebibliography}
